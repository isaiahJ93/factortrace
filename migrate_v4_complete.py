#!/usr/bin/env python3
"""
Production-Ready ESRS E1 Migration Script v4 - COMPLETE
All edge cases handled for bulletproof operation
"""

import os
import sys
import subprocess
import shutil
import json
import ast
import astor
import time
import signal
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple, Set, Optional, Any
import logging
from dataclasses import dataclass, field
from collections import defaultdict
import hashlib
import functools

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.FileHandler('migration.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Debug logger
debug_logger = logging.getLogger(f"{__name__}.debug")
debug_handler = logging.FileHandler('migration_debug.log')
debug_handler.setLevel(logging.DEBUG)
debug_logger.addHandler(debug_handler)
debug_logger.setLevel(logging.DEBUG)

@dataclass
class ModuleRoute:
    """Defines routing rule for AST nodes"""
    target_path: str
    rule: callable
    description: str

@dataclass
class SafetyCheckpoint:
    """Track safety validation at each step"""
    step: int
    name: str
    status: str = "pending"
    details: Dict = field(default_factory=dict)

class ProductionMigrator:
    def __init__(self, source_file: str = 'esrs_e1_full.py', 
                 config_file: str = 'migration_config.json'):
        self.source_file = source_file
        self.config_file = config_file
        self.target_dir = 'factortrace'
        self.checkpoints: List[SafetyCheckpoint] = []
        self.original_hash = None
        self.test_snapshot = None
        
        # Load or create default config
        self.config = self.load_config()
        
        # Track performance
        self.perf_stats = {}
        
        # Setup signal handler for graceful exit
        signal.signal(signal.SIGINT, self._handle_interrupt)
        self._interrupted = False
    
    def _handle_interrupt(self, signum, frame):
        """Handle Ctrl-C gracefully"""
        logger.warning("\n‚ö†Ô∏è  Interrupt received! Starting graceful rollback...")
        self._interrupted = True
        self.rollback()
        sys.exit(130)  # Standard exit code for SIGINT
    
    def load_config(self) -> Dict[str, Any]:
        """Load migration configuration"""
        if os.path.exists(self.config_file):
            with open(self.config_file, 'r') as f:
                return json.load(f)
        
        # Default minimal structure
        return {
            "directories": [
                "api",          # FastAPI routers
                "services",     # Business logic  
                "schemas",      # Pydantic models
                "core",         # Core functionality
                "validators",   # Validation logic
                "generators",   # Report generation
                "utils"         # Utilities
            ],
            "module_rules": {
                "api/router.py": {
                    "functions": ["generate_esrs_e1_report"],
                },
                "schemas/models.py": {
                    "base_classes": ["Enum", "BaseModel", "str"],
                    "name_patterns": [".*Model$", ".*Schema$", ".*Enum$"]
                },
                "validators/main.py": {
                    "name_patterns": ["^validate_.*", ".*_validator$"]
                },
                "generators/xbrl.py": {
                    "name_patterns": [".*xbrl.*", ".*tag.*", "create_.*tag"]
                },
                "core/constants.py": {
                    "type": "constants",
                    "name_patterns": ["^[A-Z_]+$"]
                },
                "core/large_data.py": {
                    "type": "large_constants"
                }
            }
        }
    
    def run_command(self, cmd: str, check: bool = True) -> Tuple[int, str, str]:
        """Run shell command with logging"""
        logger.info(f"Running: {cmd}")
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
        
        # Log output at debug level
        if result.stdout:
            debug_logger.debug(f"stdout: {result.stdout}")
        if result.stderr:
            debug_logger.debug(f"stderr: {result.stderr}")
        
        if check and result.returncode != 0:
            logger.error(f"Command failed: {cmd}")
            logger.error(f"stderr: {result.stderr}")
            raise RuntimeError(f"Command failed: {cmd}")
            
        return result.returncode, result.stdout, result.stderr
    
    def normalize_path(self, path: str) -> str:
        """Normalize path for cross-platform compatibility"""
        return Path(path).as_posix()
    
    def checkpoint(self, step: int, name: str, status: str = "success", details: Dict = None):
        """Record safety checkpoint"""
        cp = SafetyCheckpoint(step, name, status, details or {})
        self.checkpoints.append(cp)
        
        icon = "‚úÖ" if status == "success" else "‚ùå" if status == "failed" else "‚è≥"
        logger.info(f"{icon} Step {step}: {name} - {status}")
        
        if status == "failed":
            self.rollback()
            raise RuntimeError(f"Step {step} failed: {name}")
    
    def rollback(self):
        """Emergency rollback procedure"""
        logger.warning("üö® ROLLBACK INITIATED")
        
        # Save detailed failure report
        failure_report = {
            'timestamp': datetime.now().isoformat(),
            'checkpoints': [vars(cp) for cp in self.checkpoints],
            'source_file': self.source_file,
            'last_error': self.checkpoints[-1].details if self.checkpoints else {}
        }
        
        with open('migration_failure.json', 'w') as f:
            json.dump(failure_report, f, indent=2)
        
        # Git reset
        self.run_command("git reset --hard HEAD", check=False)
        self.run_command("git checkout main", check=False)
        
        logger.info("Rollback complete. Check migration_failure.json for details.")
    
    # STEP 1: Branch & Freeze
    def step1_branch_and_freeze(self):
        """Create branch and backup tag"""
        logger.info("=" * 60)
        logger.info("STEP 1: Branch & Freeze")
        
        # Check for uncommitted changes
        ret, stdout, _ = self.run_command("git status --porcelain")
        if stdout.strip():
            self.checkpoint(1, "Branch & Freeze", "failed", 
                          {"error": "Uncommitted changes detected"})
        
        # Create branch
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        branch_name = f"refactor/split-monolith-{timestamp}"
        
        self.run_command(f"git checkout -b {branch_name}")
        self.run_command(f"git tag pre-split-backup-{timestamp}")
        
        # Calculate file hash for integrity check
        with open(self.source_file, 'rb') as f:
            self.original_hash = hashlib.sha256(f.read()).hexdigest()
        
        self.checkpoint(1, "Branch & Freeze", "success", {
            "branch": branch_name,
            "tag": f"pre-split-backup-{timestamp}",
            "file_hash": self.original_hash
        })
    
    # STEP 2: Lock Behavior with dynamic module name
    def step2_lock_behavior(self):
        """Create and run smoke tests in subprocess"""
        logger.info("=" * 60)
        logger.info("STEP 2: Lock Behavior")
        
        # Extract module name from source file
        module_name = Path(self.source_file).stem
        
        # Create test file with dynamic imports
        test_content = f'''
import pytest
import json
import sys
from pathlib import Path

# Module name from migration
SOURCE_MODULE = "{module_name}"

# Dynamic import that works before and after split
def get_report_generator():
    """Import report generator from wherever it lives"""
    try:
        # Post-split location
        from factortrace.services.report_builder import create_enhanced_ixbrl_structure
        return create_enhanced_ixbrl_structure
    except ImportError:
        # Pre-split location - dynamic module name
        sys.path.insert(0, str(Path(__file__).parent))
        module = __import__(SOURCE_MODULE)
        return module.create_enhanced_ixbrl_structure

def test_build_report():
    """Test report generation with real payload"""
    create_report = get_report_generator()
    
    payload = {{
        "organization": "Test Corp",
        "lei": "123456789012345678901",
        "reporting_period": 2023,
        "emissions": {{
            "scope1": 1000,
            "scope2_location": 500,
            "scope2_market": 400,
            "scope3_total": 5000
        }},
        "energy": {{
            "total_mwh": 2000,
            "renewable_percentage": 45
        }},
        "targets": {{
            "base_year": 2020,
            "targets": [{{
                "year": 2030,
                "reduction": 50,
                "scope": "Scope 1+2"
            }}]
        }}
    }}
    
    # Generate report
    result = create_report(payload)
    
    # Verify it's valid XML
    import xml.etree.ElementTree as ET
    root = ET.fromstring(result)
    assert root is not None
    assert 'html' in root.tag
    
    # Store snapshot for comparison
    with open('.test_snapshot.xml', 'w') as f:
        f.write(result)
    
    # Calculate hash for later comparison
    import hashlib
    return hashlib.sha256(result.encode()).hexdigest()

def test_endpoint():
    """Test API endpoint (if available)"""
    try:
        from fastapi.testclient import TestClient
        from fastapi import FastAPI
        
        # Try both import locations
        try:
            from factortrace.api.router import router
        except ImportError:
            module = __import__(SOURCE_MODULE)
            router = module.router
            
        app = FastAPI()
        app.include_router(router)
        client = TestClient(app)
        
        response = client.post(
            "/reports/generate",
            json={{
                "organization": "Test Corp",
                "lei": "123456789012345678901",
                "reporting_period": 2023,
                "emissions": {{"scope1": 1000}}
            }}
        )
        
        assert response.status_code == 202
        assert "report_id" in response.json()
        
    except (ImportError, AttributeError):
        # Skip if FastAPI not available
        pytest.skip("FastAPI not available")

if __name__ == "__main__":
    # Run and save hash
    hash_result = test_build_report()
    with open('.test_hash.txt', 'w') as f:
        f.write(hash_result)
    print(f"‚úÖ Tests passed, output hash: {{hash_result}}")
'''
        
        with open('test_smoke.py', 'w') as f:
            f.write(test_content)
        
        # Run tests in subprocess
        ret, stdout, stderr = self.run_command(
            f"{sys.executable} -m pytest -xvs test_smoke.py --tb=short", 
            check=False
        )
        
        if ret != 0:
            self.checkpoint(2, "Lock Behavior", "failed", {
                "error": "Smoke tests failed",
                "stderr": stderr
            })
        
        # Save test snapshot hash
        if os.path.exists('.test_hash.txt'):
            with open('.test_hash.txt', 'r') as f:
                self.test_snapshot = f.read().strip()
        
        self.checkpoint(2, "Lock Behavior", "success", {
            "tests_passed": True,
            "snapshot_hash": self.test_snapshot,
            "module_name": module_name
        })
    
    # STEP 3: Pin Environment
    def step3_pin_environment(self):
        """Lock all dependencies"""
        logger.info("=" * 60)
        logger.info("STEP 3: Pin Environment")
        
        self.run_command("pip freeze > requirements.lock")
        
        env_data = {
            "python_version": sys.version,
            "platform": sys.platform,
            "timestamp": datetime.now().isoformat()
        }
        
        with open('.environment.json', 'w') as f:
            json.dump(env_data, f, indent=2)
        
        self.checkpoint(3, "Pin Environment", "success", env_data)
    
    # STEP 4: Snapshot Import Graph
    def step4_snapshot_imports(self):
        """Create import graph baseline"""
        logger.info("=" * 60)
        logger.info("STEP 4: Snapshot Import Graph")
        
        # Generate import graph with JSON output
        self.run_command("python -m grimp build . --format json --output pre_split.json")
        
        # Also create our own import analysis
        imports = self.analyze_imports(self.source_file)
        with open('imports_baseline.json', 'w') as f:
            json.dump(imports, f, indent=2)
        
        # Extract any existing cycles
        cycles = self.find_cycles('pre_split.json')
        
        self.checkpoint(4, "Snapshot Import Graph", "success", {
            "import_count": len(imports),
            "existing_cycles": len(cycles),
            "graph_created": True
        })
    
    # STEP 5: Generate Skeleton
    def step5_generate_skeleton(self):
        """Create minimal directory structure"""
        logger.info("=" * 60)
        logger.info("STEP 5: Generate Skeleton")
        
        # Use configured directories only
        directories = self.config['directories']
        
        for dir_name in directories:
            dir_path = os.path.join(self.target_dir, dir_name)
            os.makedirs(dir_path, exist_ok=True)
            
            # Create __init__.py
            init_path = os.path.join(dir_path, '__init__.py')
            with open(init_path, 'w') as f:
                f.write(f'"""{dir_name.capitalize()} module for ESRS E1."""\n')
        
        # Create core/bootstrap.py for side effects
        bootstrap_path = os.path.join(self.target_dir, 'core', 'bootstrap.py')
        with open(bootstrap_path, 'w') as f:
            f.write('''"""
Bootstrap module for initialization and side effects.
All top-level code from the monolith should be moved here.
"""

import logging
import os
from typing import Dict, Any

# Configure logging
logging.basicConfig(level=logging.INFO)

# Environment configuration
CONFIG: Dict[str, Any] = {
    "debug": os.getenv("DEBUG", "false").lower() == "true",
    "api_timeout": int(os.getenv("API_TIMEOUT", "30")),
}

# Other initialization code will be moved here
''')
        
        # Also create core/unsorted.py for unmatched nodes
        unsorted_path = os.path.join(self.target_dir, 'core', 'unsorted.py')
        with open(unsorted_path, 'w') as f:
            f.write('"""Temporary home for code that needs proper categorization."""\n\n')
        
        self.checkpoint(5, "Generate Skeleton", "success", {
            "directories_created": len(directories) + 1  # +1 for bootstrap
        })
    
    # STEP 6: Enhanced AST Split
    def step6_ast_split(self):
        """Split using AST with proper import preservation"""
        logger.info("=" * 60)
        logger.info("STEP 6: AST-based Split")
        
        # Performance warning for large files
        file_size = os.path.getsize(self.source_file)
        if file_size > 500_000:  # 500KB
            logger.warning(f"Large file detected ({file_size/1024:.1f}KB). "
                         f"AST processing may take 15-30 seconds...")
        
        start_time = time.time()
        
        # Parse source file
        with open(self.source_file, 'r') as f:
            source = f.read()
        
        # Extract future imports first
        future_imports = self.extract_future_imports(source)
        
        try:
            tree = ast.parse(source)
        except SyntaxError as e:
            self.checkpoint(6, "AST Split", "failed", {"error": str(e)})
        
        parse_time = time.time() - start_time
        self.perf_stats['parse_time'] = parse_time
        
        # Analyze AST
        analyzer = EnhancedASTAnalyzer()
        analyzer.visit(tree)
        
        # Extract top-level side effects for bootstrap
        side_effects = self.extract_side_effects(tree)
        if side_effects:
            self.write_bootstrap_additions(side_effects, future_imports)
        
        # Build routing rules with proper closures
        routes = self.build_routes_fixed()
        
        # Route all nodes
        routed_nodes = self.route_nodes(analyzer, routes)
        
        # Write modules with preserved imports
        write_start = time.time()
        files_created = self.write_modules_with_imports(
            routed_nodes, analyzer, future_imports
        )
        write_time = time.time() - write_start
        self.perf_stats['write_time'] = write_time
        
        self.checkpoint(6, "AST Split", "success", {
            "nodes_processed": len(analyzer.all_nodes),
            "files_created": files_created,
            "unrouted_nodes": len(routed_nodes.get(self.normalize_path('core/unsorted.py'), [])),
            "side_effects_extracted": len(side_effects),
            "parse_time_seconds": round(parse_time, 2),
            "write_time_seconds": round(write_time, 2)
        })
    
    # STEP 7: Static Analysis
    def step7_static_analysis(self):
        """Run linters and type checkers"""
        logger.info("=" * 60)
        logger.info("STEP 7: Static Analysis")
        
        issues = []
        
        # Ruff check (allow some issues for now)
        ret, stdout, stderr = self.run_command(
            f"ruff check {self.target_dir} --select E,W,F --ignore E501", 
            check=False
        )
        if ret != 0 and "E999" in stderr:  # E999 is syntax error
            issues.append(("ruff", "syntax errors", stderr))
        
        # MyPy check (lenient mode)
        ret, stdout, stderr = self.run_command(
            f"mypy {self.target_dir} --ignore-missing-imports --no-strict-optional", 
            check=False
        )
        if "error:" in stderr:
            critical_errors = [l for l in stderr.split('\n') if 'error:' in l and 'import' not in l]
            if critical_errors:
                issues.append(("mypy", "type errors", '\n'.join(critical_errors[:5])))
        
        # Generate post-split graph with JSON format
        self.run_command(f"python -m grimp build {self.target_dir} --format json --output post_split.json")
        
        if issues:
            logger.warning(f"Static analysis found {len(issues)} issue categories")
            for tool, category, detail in issues:
                logger.warning(f"{tool}: {category}")
        
        self.checkpoint(7, "Static Analysis", "success", {
            "issues": len(issues),
            "tools_run": ["ruff", "mypy", "grimp"]
        })
    
    # STEP 8: Diff Graphs
    def step8_diff_graphs(self):
        """Check for circular dependencies using real cycle detection"""
        logger.info("=" * 60)
        logger.info("STEP 8: Diff Import Graphs")
        
        # Find cycles in both graphs
        pre_cycles = self.find_cycles('pre_split.json')
        post_cycles = self.find_cycles('post_split.json')
        
        new_cycles = post_cycles - pre_cycles
        
        if new_cycles:
            self.checkpoint(8, "Diff Graphs", "failed", {
                "new_cycles": list(new_cycles),
                "suggestion": "Check module boundaries and move shared code to core/"
            })
        
        self.checkpoint(8, "Diff Graphs", "success", {
            "pre_cycles": len(pre_cycles),
            "post_cycles": len(post_cycles),
            "new_cycles": 0
        })
    
    # STEP 9: Re-run Tests
    def step9_rerun_tests(self):
        """Verify functional equivalence after split"""
        logger.info("=" * 60)
        logger.info("STEP 9: Re-run Tests")
        
        # Run tests in fresh subprocess
        ret, stdout, stderr = self.run_command(
            f"{sys.executable} -m pytest -xvs test_smoke.py --tb=short", 
            check=False
        )
        
        if ret != 0:
            self.checkpoint(9, "Re-run Tests", "failed", {
                "stderr": stderr,
                "hint": "Check import paths in the test file"
            })
        
        # Verify output hash matches
        if os.path.exists('.test_hash.txt'):
            with open('.test_hash.txt', 'r') as f:
                new_hash = f.read().strip()
                
            if new_hash != self.test_snapshot:
                self.checkpoint(9, "Re-run Tests", "failed", {
                    "error": "Output changed after split",
                    "original_hash": self.test_snapshot,
                    "new_hash": new_hash
                })
        
        self.checkpoint(9, "Re-run Tests", "success", {
            "all_passed": True,
            "output_unchanged": True
        })
    
    # STEP 10: Commit
    def step10_commit(self):
        """Final commit with verification"""
        logger.info("=" * 60)
        logger.info("STEP 10: Commit")
        
        # Generate comprehensive report
        report = {
            "timestamp": datetime.now().isoformat(),
            "source_file": self.source_file,
            "target_directory": self.target_dir,
            "checkpoints": [vars(cp) for cp in self.checkpoints],
            "files_created": self.count_files(self.target_dir),
            "original_hash": self.original_hash,
            "test_snapshot": self.test_snapshot,
            "migration_config": self.config
        }
        
        with open('migration_report.json', 'w') as f:
            json.dump(report, f, indent=2)
        
        # Create quick reference
        self.create_migration_summary(report)
        
        # Git operations
        self.run_command("git add .")
        self.run_command('git commit -m "Modular split (tests green, no new cycles)"')
        
        self.checkpoint(10, "Commit", "success", {
            "files_added": report["files_created"],
            "report_generated": True
        })
        
        # Success!
        logger.info("=" * 60)
        logger.info("üéâ MIGRATION SUCCESSFUL!")
        logger.info(f"‚úÖ All 10 steps completed successfully")
        logger.info(f"üìÅ Files created in: {self.target_dir}/")
        logger.info(f"üìÑ Report: migration_report.json")
        logger.info(f"üìã Summary: MIGRATION_SUMMARY.md")
        logger.info("\nNext steps:")
        logger.info("1. Review files in core/unsorted.py")
        logger.info("2. Update imports in main application")
        logger.info("3. Run full test suite")
        logger.info("4. Merge when ready")
        logger.info("=" * 60)
    
    # Helper methods
    def extract_future_imports(self, source: str) -> List[str]:
        """Extract __future__ imports from source"""
        future_imports = []
        for line in source.split('\n'):
            if line.strip().startswith('from __future__ import'):
                future_imports.append(line.strip())
            elif line.strip() and not line.strip().startswith('#'):
                # Stop at first non-comment, non-future import
                break
        return future_imports
    
    def extract_side_effects(self, tree: ast.AST) -> List[ast.AST]:
        """Extract top-level code that has side effects"""
        side_effects = []
        
        for node in tree.body:
            # Keep imports separate - they'll be handled per-module
            if isinstance(node, (ast.Import, ast.ImportFrom)):
                continue
                
            # Functions and classes are definitions, not side effects
            if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)):
                continue
                
            # Constants are ok at module level
            if isinstance(node, ast.Assign):
                if all(isinstance(t, ast.Name) and t.id.isupper() for t in node.targets):
                    continue
            
            # Everything else is a side effect
            side_effects.append(node)
        
        return side_effects
    
    def write_bootstrap_additions(self, side_effects: List[ast.AST], 
                                 future_imports: List[str]):
        """Add side effects to bootstrap module"""
        if not side_effects:
            return
            
        bootstrap_path = os.path.join(self.target_dir, 
                                     self.normalize_path('core/bootstrap.py'))
        
        # Read existing content
        with open(bootstrap_path, 'r') as f:
            existing = f.read()
        
        # Add future imports if any
        additions = ""
        if future_imports:
            additions += "\n".join(future_imports) + "\n\n"
        
        # Add side effects
        additions += "\n# Side effects from monolith\n"
        additions += astor.to_source(ast.Module(body=side_effects))
        
        with open(bootstrap_path, 'w') as f:
            f.write(existing + additions)
    
    def analyze_imports(self, filepath: str) -> Dict:
        """Analyze imports in a file"""
        imports = defaultdict(list)
        
        with open(filepath, 'r') as f:
            tree = ast.parse(f.read())
        
        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for alias in node.names:
                    imports['import'].append(alias.name)
            elif isinstance(node, ast.ImportFrom):
                imports['from'].append({
                    'module': node.module,
                    'names': [alias.name for alias in node.names]
                })
        
        return dict(imports)
    
    def find_cycles(self, graph_file: str) -> Set[str]:
        """Parse grimp JSON output to find actual cycles"""
        if not os.path.exists(graph_file):
            return set()
            
        with open(graph_file, 'r') as f:
            data = json.load(f)
        
        cycles = set()
        
        # Grimp format: {"cycles": [["module.a", "module.b", "module.a"], ...]}
        for cycle in data.get("cycles", []):
            # Convert cycle list to string representation
            cycle_str = " -> ".join(cycle)
            cycles.add(cycle_str)
        
        return cycles
    
    def build_routes_fixed(self) -> List[ModuleRoute]:
        """Build routing rules with proper closure handling"""
        routes = []
        
        for target_path, rules in self.config['module_rules'].items():
            # Normalize target path
            target_path = self.normalize_path(target_path)
            
            # Function name matching - FIXED for async
            if 'functions' in rules:
                for func_name in rules['functions']:
                    routes.append(ModuleRoute(
                        target_path,
                        functools.partial(self._match_function_or_async, name=func_name),
                        f"Function named {func_name}"
                    ))
            
            # Pattern matching
            if 'name_patterns' in rules:
                import re
                for pattern in rules['name_patterns']:
                    regex = re.compile(pattern)
                    routes.append(ModuleRoute(
                        target_path,
                        functools.partial(self._match_pattern, regex=regex),
                        f"Name matching {pattern}"
                    ))
            
            # Base class matching
            if 'base_classes' in rules:
                for base_class in rules['base_classes']:
                    routes.append(ModuleRoute(
                        target_path,
                        functools.partial(self._match_base_class, base_name=base_class),
                        f"Class inheriting from {base_class}"
                    ))
            
            # Constants
            if rules.get('type') == 'constants':
                routes.append(ModuleRoute(
                    target_path,
                    self._match_constant,
                    "Uppercase constants"
                ))
            
            # Large constants (new)
            if rules.get('type') == 'large_constants':
                routes.append(ModuleRoute(
                    target_path,
                    self._match_large_constant,
                    "Large string constants"
                ))
        
        return routes
    
    @staticmethod
    def _match_function_or_async(node: ast.AST, name: str) -> bool:
        """Match function or async function by name"""
        return (isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)) and 
                node.name == name)
    
    @staticmethod
    def _match_pattern(node: ast.AST, regex) -> bool:
        """Match node name by pattern"""
        return hasattr(node, 'name') and regex.match(node.name)
    
    @staticmethod
    def _match_base_class(node: ast.AST, base_name: str) -> bool:
        """Match class by base class"""
        if not isinstance(node, ast.ClassDef):
            return False
        
        for base in node.bases:
            if isinstance(base, ast.Name) and base.id == base_name:
                return True
            elif isinstance(base, ast.Attribute) and base.attr == base_name:
                return True
        return False
    
    @staticmethod
    def _match_constant(node: ast.AST) -> bool:
        """Match uppercase constants"""
        return (isinstance(node, ast.Assign) and
                all(isinstance(t, ast.Name) and t.id.isupper() 
                    for t in node.targets))
    
    @staticmethod
    def _match_large_constant(node: ast.AST) -> bool:
        """Match large string constants that might break astor"""
        if isinstance(node, ast.Assign):
            for value in ast.walk(node.value):
                if isinstance(value, ast.Constant) and isinstance(value.value, str):
                    if len(value.value) > 50000:  # 50KB strings
                        return True
        return False
    
    def route_nodes(self, analyzer: 'EnhancedASTAnalyzer', 
                    routes: List[ModuleRoute]) -> Dict[str, List[ast.AST]]:
        """Route nodes to target modules with normalized paths"""
        routed = defaultdict(list)
        
        for node in analyzer.all_nodes:
            matched = False
            
            for route in routes:
                if route.rule(node):
                    routed[route.target_path].append(node)
                    matched = True
                    break
            
            if not matched:
                # Default to unsorted with normalized path
                unsorted_path = self.normalize_path('core/unsorted.py')
                routed[unsorted_path].append(node)
                logger.warning(f"Unmatched node: {type(node).__name__} "
                             f"{getattr(node, 'name', 'unnamed')}")
        
        return dict(routed)
    
    def determine_needed_imports(self, nodes: List[ast.AST], 
                                 analyzer: 'EnhancedASTAnalyzer') -> List[ast.AST]:
        """Determine imports needed including nested ones"""
        # Collect all names used in nodes (including nested)
        name_collector = NameCollector()
        for node in nodes:
            name_collector.visit(node)
        
        # Also collect imports defined within functions
        import_collector = NestedImportCollector()
        for node in nodes:
            import_collector.visit(node)
        
        used_names = name_collector.used_names
        
        # Find imports that provide these names
        needed_imports = []
        seen_imports = set()
        
        # Check top-level imports
        for imp in analyzer.imports:
            imp_str = astor.to_source(imp).strip()
            if imp_str in seen_imports:
                continue
                
            if isinstance(imp, ast.Import):
                for alias in imp.names:
                    import_name = alias.asname or alias.name
                    if import_name in used_names:
                        needed_imports.append(imp)
                        seen_imports.add(imp_str)
                        break
                        
            elif isinstance(imp, ast.ImportFrom):
                for alias in imp.names:
                    import_name = alias.asname or alias.name
                    if import_name in used_names:
                        # Only include this specific import
                        new_imp = ast.ImportFrom(
                            module=imp.module,
                            names=[alias],
                            level=imp.level
                        )
                        new_imp_str = astor.to_source(new_imp).strip()
                        if new_imp_str not in seen_imports:
                            needed_imports.append(new_imp)
                            seen_imports.add(new_imp_str)
        
        # Add nested imports found
        for imp in import_collector.imports:
            imp_str = astor.to_source(imp).strip()
            if imp_str not in seen_imports:
                needed_imports.append(imp)
                seen_imports.add(imp_str)
        
        return needed_imports
    
    def needs_bootstrap(self, nodes: List[ast.AST]) -> bool:
        """Check if nodes reference CONFIG or other bootstrap items"""
        name_collector = NameCollector()
        for node in nodes:
            name_collector.visit(node)
        
        bootstrap_names = {'CONFIG', 'logger', 'REGISTRY'}
        return bool(name_collector.used_names & bootstrap_names)
    
    def write_large_constant(self, node: ast.Assign) -> str:
        """Write large constants safely"""
        # Find the string value
        for value in ast.walk(node.value):
            if isinstance(value, ast.Constant) and isinstance(value.value, str):
                if len(value.value) > 50000:
                    # Write as triple-quoted string
                    target_name = node.targets[0].id if node.targets else "LARGE_CONST"
                    
                    # Use repr to escape the string safely
                    escaped = repr(value.value)
                    
                    # Format as triple-quoted
                    return f'{target_name} = """{value.value}"""\n'
        
        # Fallback to normal serialization
        return astor.to_source(node)
    
    def write_modules_with_imports(self, routed_nodes: Dict[str, List[ast.AST]], 
                                   analyzer: 'EnhancedASTAnalyzer',
                                   future_imports: List[str]) -> int:
        """Write modules preserving necessary imports"""
        files_created = 0
        
        # First pass: determine which imports each module needs
        module_imports = {}
        for module_path, nodes in routed_nodes.items():
            needed_imports = self.determine_needed_imports(nodes, analyzer)
            module_imports[module_path] = needed_imports
        
        # Second pass: write modules
        for module_path, nodes in routed_nodes.items():
            if not nodes:
                continue
                
            filepath = os.path.join(self.target_dir, module_path)
            os.makedirs(os.path.dirname(filepath), exist_ok=True)
            
            # Create module
            module_lines = []
            
            # Header
            module_lines.append(f'"""{module_path} - Auto-generated from {self.source_file}"""')
            
            # Future imports
            if future_imports:
                module_lines.extend(future_imports)
            
            # Regular imports
            imports = module_imports[module_path]
            if imports:
                import_code = astor.to_source(ast.Module(body=imports))
                module_lines.append(import_code)
            
            # Bootstrap import if needed
            if module_path != self.normalize_path('core/bootstrap.py') and self.needs_bootstrap(nodes):
                # Calculate relative import level based on module depth
                module_depth = module_path.count('/')
                if module_depth == 0:
                    # Same level as core
                    bootstrap_import = "from .core.bootstrap import CONFIG"
                else:
                    # Calculate dots needed
                    dots = '.' * (module_depth + 1)
                    bootstrap_import = f"from {dots}core.bootstrap import CONFIG"
                
                module_lines.append(bootstrap_import)
            
            # Add nodes - handle large constants specially
            for node in nodes:
                if self._match_large_constant(node):
                    module_lines.append(self.write_large_constant(node))
                else:
                    module_lines.append(astor.to_source(ast.Module(body=[node])))
            
            # Write file
            with open(filepath, 'w') as f:
                f.write('\n\n'.join(module_lines))
            
            files_created += 1
            logger.info(f"Created {module_path} with {len(nodes)} definitions")
        
        return files_created
    
    def count_files(self, directory: str) -> int:
        """Count Python files created"""
        count = 0
        for root, dirs, files in os.walk(directory):
            count += sum(1 for f in files if f.endswith('.py'))
        return count
    
    def create_migration_summary(self, report: Dict[str, Any]):
        """Create human-readable summary"""
        source_module = Path(self.source_file).stem
        
        # Build summary without problematic nested triple quotes
        summary = f"""# Migration Summary

**Generated**: {report['timestamp']}

## Results
- **Files Created**: {report['files_created']}
- **Source**: `{report['source_file']}`
- **Target**: `{report['target_directory']}/`

## Status
‚úÖ All smoke tests passing
‚úÖ No new circular dependencies
‚úÖ Output functionally identical (hash: {report['test_snapshot'][:8]}...)

## Next Steps

1. **Review Unsorted Code**
   - Check `{self.target_dir}/core/unsorted.py` for uncategorized code
   - Move to appropriate modules

2. **Update Main App**
"""
        
        # Add code block separately to avoid f-string issues
        summary += f"""
```python
# Old
from {source_module} import router

# New
from factortrace.api.router import router
```
"""
        
        summary += """
3. **Run Full Tests**
```bash
pytest tests/
```

## Module Structure
"""
        
        # Add file tree
        for root, dirs, files in os.walk(self.target_dir):
            level = root.replace(self.target_dir, '').count(os.sep)
            indent = '  ' * level
            summary += f"{indent}{os.path.basename(root)}/\n"
            subindent = '  ' * (level + 1)
            for f in files:
                if f.endswith('.py'):
                    summary += f"{subindent}{f}\n"
        
        with open('MIGRATION_SUMMARY.md', 'w') as f:
            f.write(summary)
    
    def pre_flight_check(self):
        """Verify environment before starting"""
        logger.info("üîç Running pre-flight checks...")
        
        # Create requirements file with pinned versions
        requirements = [
            'pytest>=7.0.0',
            'ruff>=0.1.0',
            'mypy>=1.0.0',
            'grimp>=3.0',
            'astor==0.8.1'  # Pinned to avoid deprecation issues
        ]
        
        with open('migration_requirements.txt', 'w') as f:
            f.write('\n'.join(requirements))
        
        # Check required tools
        required_tools = {
            'git': 'Git version control',
            'python': 'Python interpreter',
            'pip': 'Python package manager'
        }
        
        for tool, description in required_tools.items():
            if sys.platform == 'win32':
                check_cmd = f"where {tool}"
            else:
                check_cmd = f"which {tool}"
                
            ret = subprocess.run(check_cmd, shell=True, capture_output=True).returncode
            if ret != 0:
                logger.error(f"‚ùå Required tool missing: {tool} ({description})")
                sys.exit(1)
        
        # Check required packages
        missing = []
        for req in requirements:
            package = req.split('>=')[0].split('==')[0]
            ret = subprocess.run(f"pip show {package}", shell=True, capture_output=True).returncode
            if ret != 0:
                missing.append(req)
        
        if missing:
            logger.error(f"‚ùå Missing packages: {', '.join(missing)}")
            logger.error("Please run: pip install -r migration_requirements.txt")
            sys.exit(1)
        
        logger.info("‚úÖ All pre-flight checks passed")
    
    def run(self):
        """Execute all steps with interrupt handling"""
        # Pre-flight checks first
        self.pre_flight_check()
        
        steps = [
            self.step1_branch_and_freeze,
            self.step2_lock_behavior,
            self.step3_pin_environment,
            self.step4_snapshot_imports,
            self.step5_generate_skeleton,
            self.step6_ast_split,
            self.step7_static_analysis,
            self.step8_diff_graphs,
            self.step9_rerun_tests,
            self.step10_commit,
        ]
        
        try:
            for i, step in enumerate(steps, 1):
                if self._interrupted:
                    break
                    
                step()
                
                if i < len(steps):
                    logger.info(f"‚úÖ Completed step {i}/10. Continuing...")
                    
        except KeyboardInterrupt:
            logger.warning("Caught keyboard interrupt during execution")
            self.rollback()
            sys.exit(130)
        except Exception as e:
            logger.error(f"Migration failed: {e}")
            logger.error("Check migration_failure.json for details")
            sys.exit(1)


class EnhancedASTAnalyzer(ast.NodeVisitor):
    """Analyze AST with better categorization"""
    
    def __init__(self):
        self.all_nodes = []
        self.imports = []
        self.functions = []
        self.classes = []
        self.constants = []
        
    def visit_Import(self, node):
        self.imports.append(node)
        self.generic_visit(node)
    
    def visit_ImportFrom(self, node):
        self.imports.append(node)
        self.generic_visit(node)
    
    def visit_FunctionDef(self, node):
        self.functions.append(node)
        self.all_nodes.append(node)
        # Don't visit children - we want top-level only
    
    def visit_AsyncFunctionDef(self, node):
        self.functions.append(node)
        self.all_nodes.append(node)
    
    def visit_ClassDef(self, node):
        self.classes.append(node)
        self.all_nodes.append(node)
    
    def visit_Assign(self, node):
        # Only top-level assigns
        if isinstance(node.targets[0], ast.Name):
            if node.targets[0].id.isupper():
                self.constants.append(node)
            self.all_nodes.append(node)


class NameCollector(ast.NodeVisitor):
    """Collect all names used in AST nodes"""
    
    def __init__(self):
        self.used_names = set()
    
    def visit_Name(self, node):
        self.used_names.add(node.id)
        self.generic_visit(node)
    
    def visit_Attribute(self, node):
        # For things like `module.function`
        if isinstance(node.value, ast.Name):
            self.used_names.add(node.value.id)
        self.generic_visit(node)


class NestedImportCollector(ast.NodeVisitor):
    """Collect imports defined within functions"""
    
    def __init__(self):
        self.imports = []
    
    def visit_Import(self, node):
        self.imports.append(node)
        self.generic_visit(node)
    
    def visit_ImportFrom(self, node):
        self.imports.append(node)
        self.generic_visit(node)


if __name__ == '__main__':
    # Parse arguments
    import argparse
    
    parser = argparse.ArgumentParser(description='Split monolithic Python file into modules')
    parser.add_argument('source', nargs='?', default='esrs_e1_full.py',
                        help='Source file to split (default: esrs_e1_full.py)')
    parser.add_argument('--config', default='migration_config.json',
                        help='Configuration file (default: migration_config.json)')
    parser.add_argument('--format', action='store_true',
                        help='Run Black formatter after migration')
    
    args = parser.parse_args()
    
    # Run migration
    migrator = ProductionMigrator(
        source_file=args.source,
        config_file=args.config
    )
    
    migrator.run()
    
    if args.format:
        logger.info("Running Black formatter...")
        subprocess.run(f"black {migrator.target_dir}", shell=True)
        logger.info("‚úÖ Formatting complete")